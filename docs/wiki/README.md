<!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- "License"); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at -->

<!---   http://www.apache.org/licenses/LICENSE-2.0 -->

<!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. -->

[comment]: <> (Auto generated by docs/wiki/gen_readme.py. Do not touch)
## Motivation
**Meta framework.** Because, first, it is not a single framework, but meta by nature. With TVM Relay frontend importers, we are able to absort any TensorFlow model, ONNX model, into our core intermediate representation, and mix it with each other - as a researcher, just imagine that you are able to define the tedious part of your network in Keras, like backbones in vision, and we allow you to focus on manipulating the awesome part of your neural network using our numpy-like API.

**Compiler native.** Second, it is said that it is boring to develop another one because all frontends are converging. True. While we converge to Numpy + Gluon/Keras, we are the first compiler-native framework, which is the key feature that makes us the most outstanding. We are the missing frontend of TVM/Relay stack for intelligent compiler. Once you implement your model using our framework, our JIT is in charge of fusing operators, finding the best data layout for training.

**Python-Relay Transpiler.** Third, we are capable of translating Python to the properly-designed intermediate representation, Relay, supporting arbitrary control flow (nested loop/if/continue/break/return), mutual function calls and wide range of types, including primitive types\* and container types (tuple, list, dict), and classes\*\* as well as back propogation through them. In the meantime, we are able to hook in almost all developer/user-defined functions to enrich our system via either true hybridization or packed function registration.

\* Integers, floating point numbers, bool, and potentially strings.

\*\* Technically, we support class through named tuple, which is slightly different.





## Table of Contents
- Getting Start
    - [Build On Conda](1_getting_start/Build-on-Conda.md)
    - [Build On MacOS](1_getting_start/Build-on-macOS.md)
    - [Build On Ubuntu 18.04](1_getting_start/Build-on-Ubuntu-18.04.md)
- User Guide
    - [AMP](2_user_guide/AMP.md)
    - [Distributed Training](2_user_guide/Distributed-Training.md)
    - [Train Model](2_user_guide/Train-Model.md)
    - [Train PyTorch Model](2_user_guide/Train-PyTorch-Model.md)
- Dev Guide
    - [Add Operator](3_dev_guide/Add-Operator.md)
    - [Add Pass](3_dev_guide/Add-Pass.md)
    - [Distributed Training](3_dev_guide/Distributed-Training.md)
    - [IR Format](3_dev_guide/IR-Format.md)
    - [Memory Pool](3_dev_guide/Memory-Pool.md)
    - [Profile Model](3_dev_guide/Profile-Model.md)
- Contrib Guide
    - [Code Review](4_contrib_guide/Code-Review.md)
    - [Development And Pull Request](4_contrib_guide/Development-And-Pull-Request.md)
    - [Docker](4_contrib_guide/Docker.md)
    - [RFC Collections](4_contrib_guide/RFC-Collections.md)
